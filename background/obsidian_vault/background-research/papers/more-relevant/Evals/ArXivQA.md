# Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models
## Reference

https://arxiv.org/abs/2403.00231

## Summary

They build the ArXivCap dataset which is a figure-caption dataset comprising 6.4M images and 3.9M captions, sourced from 572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap, they introduce ArXivQA, a question-answering dataset generated by prompting GPT-4V based on scientific figures.

## Notes

Process to build the dataset:
- Extract ArXiv tar files before june 2023
- Filter to keep only JournalArticle, Conference, or Review (assume the peer-review process could ensure the overall figure-caption quality is satisfactory) using metadata of papers from Semantic Scholar.
- Figure-Caption Pair Extraction:
	- Images and captions extracted from the original LaTeX files by matching the syntax.
	- Use a robust ImageMagisk (ImageMagick Studio LLC) to convert images into JPEG format for easy processing.
	- Extracted images and captions stored in a chunk structure:
		- A single figure-caption pair or multiple figures with their respective sub-captions and a main caption for the overall description.
	- Additional filtering on each chunk:
		- Chunks with captions shorter than 5 words are removed.
		- Use pylatexenc to handle Latex math formulas
		- Remove images if:
			- Images with an aspect ratio larger than 100
			- Images with the shortest edge shorter than 224 pixels
			- Images with pixel numbers larger than the decompression bombs threshold.
	- Small amount sampled to perform an additional manual inspection, showcased good extraction.
- Prompt GPT to generate questions, and answers, for each chunk.
