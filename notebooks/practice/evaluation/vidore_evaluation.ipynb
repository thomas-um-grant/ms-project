{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "from typing import List, Optional, Union, cast\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from vidore_benchmark.evaluation.vidore_evaluators import ViDoReEvaluatorQA\n",
    "from vidore_benchmark.retrievers import BaseVisionRetriever\n",
    "from vidore_benchmark.utils.data_utils import ListDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_torch_device(device: str = \"auto\") -> str:\n",
    "    \"\"\"\n",
    "    Returns the device (string) to be used by PyTorch.\n",
    "\n",
    "    `device` arg defaults to \"auto\" which will use:\n",
    "    - \"cuda:0\" if available\n",
    "    - else \"mps\" if available\n",
    "    - else \"cpu\".\n",
    "    \"\"\"\n",
    "\n",
    "    if device == \"auto\":\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        elif torch.backends.mps.is_available():  # for Apple Silicon\n",
    "            device = \"mps\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "def tear_down_torch():\n",
    "    \"\"\"\n",
    "    Teardown for PyTorch.\n",
    "    Clears GPU cache for both CUDA and MPS.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericRetriever(BaseVisionRetriever):\n",
    "    \"\"\"\n",
    "    Generic retriever, based on retriever used in the ViDoRe benchmark.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model_name_or_path: str = \"vidore/colpali-v1.3\",\n",
    "        device: str = \"auto\",\n",
    "        num_workers: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(use_visual_embedding=True)\n",
    "\n",
    "        try:\n",
    "            from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                'Install the missing dependencies with `pip install \"vidore-benchmark[colpali-engine]\"` '\n",
    "                \"to use ColPaliRetriever.\"\n",
    "            )\n",
    "\n",
    "        self.device = get_torch_device(device)\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # Load the model\n",
    "        self.model = cast(\n",
    "            ColPali,\n",
    "            ColPali.from_pretrained(\n",
    "                pretrained_model_name_or_path,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=self.device,\n",
    "            ).eval(),\n",
    "        )\n",
    "\n",
    "        # Load the processor\n",
    "        self.processor = cast(\n",
    "            ColPaliProcessor,\n",
    "            ColPaliProcessor.from_pretrained(pretrained_model_name_or_path),\n",
    "        )\n",
    "\n",
    "    def process_images(self, images: List[Image.Image], **kwargs):\n",
    "        return self.processor.process_images(images=images)\n",
    "\n",
    "    def process_queries(self, queries: List[str], **kwargs):\n",
    "        return self.processor.process_queries(queries=queries)\n",
    "\n",
    "    def forward_queries(\n",
    "        self, queries: List[str], batch_size: int, **kwargs\n",
    "    ) -> List[torch.Tensor]:\n",
    "        dataloader = DataLoader(\n",
    "            dataset=ListDataset[str](queries),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.process_queries,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "        query_embeddings: List[torch.Tensor] = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # for batch_query in tqdm(dataloader, desc=\"Forward pass queries...\", leave=False):\n",
    "            #     embeddings_query = self.model(**batch_query).to(\"cpu\")\n",
    "            #     query_embeddings.extend(list(torch.unbind(embeddings_query)))\n",
    "            for batch_query in tqdm(\n",
    "                dataloader, desc=\"Forward pass queries...\", leave=False\n",
    "            ):\n",
    "                batch_query = batch_query.to(self.device)\n",
    "                query_embeddings = self.model(**batch_query).to(\"cpu\")\n",
    "\n",
    "        return query_embeddings\n",
    "\n",
    "    def forward_passages(\n",
    "        self, passages: List[Image.Image], batch_size: int, **kwargs\n",
    "    ) -> List[torch.Tensor]:\n",
    "        dataloader = DataLoader(\n",
    "            dataset=ListDataset[Image.Image](passages),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.process_images,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "        passage_embeddings: List[torch.Tensor] = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # for batch_doc in tqdm(dataloader, desc=\"Forward pass documents...\", leave=False):\n",
    "            #     embeddings_doc = self.model(**batch_doc).to(\"cpu\")\n",
    "            #     passage_embeddings.extend(list(torch.unbind(embeddings_doc)))\n",
    "            for batch_doc in tqdm(\n",
    "                dataloader, desc=\"Forward pass documents...\", leave=False\n",
    "            ):\n",
    "                batch_doc = batch_doc.to(self.device)\n",
    "                passage_embeddings = self.model(**batch_doc).to(\"cpu\")\n",
    "\n",
    "        return passage_embeddings\n",
    "\n",
    "    def get_scores(\n",
    "        self,\n",
    "        query_embeddings: Union[torch.Tensor, List[torch.Tensor]],\n",
    "        passage_embeddings: Union[torch.Tensor, List[torch.Tensor]],\n",
    "        batch_size: Optional[int] = 128,\n",
    "    ) -> torch.Tensor:\n",
    "        if batch_size is None:\n",
    "            raise ValueError(\n",
    "                \"`batch_size` must be provided for ColPaliRetriever's scoring\"\n",
    "            )\n",
    "        scores = self.processor.score(\n",
    "            query_embeddings,\n",
    "            passage_embeddings,\n",
    "            batch_size=batch_size,\n",
    "            device=\"cpu\",\n",
    "        )\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.GenericRetriever'>\n"
     ]
    }
   ],
   "source": [
    "print(GenericRetriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047d7eb05db64018ba81ea90f3e320d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f32dd4fe6d447782ab5c3f709db1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c514dda5964272a608b8f645b0bbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataloader pre-batching for passages:   0%|          | 0/7 [00:00<?, ?it/s]       "
     ]
    }
   ],
   "source": [
    "# Setup retriever\n",
    "genericRetriever = GenericRetriever(\n",
    "    pretrained_model_name_or_path=\"vidore/colpali-v1.3\",\n",
    "    device=\"auto\",\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# Evaluate on a single QA format dataset\n",
    "vidore_evaluator_qa = ViDoReEvaluatorQA(genericRetriever)\n",
    "ds = load_dataset(\"vidore/tabfquad_test_subsampled\", split=\"test\")\n",
    "metrics_dataset_qa = vidore_evaluator_qa.evaluate_dataset(\n",
    "    ds=ds, batch_query=4, batch_passage=4\n",
    ")\n",
    "print(metrics_dataset_qa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
